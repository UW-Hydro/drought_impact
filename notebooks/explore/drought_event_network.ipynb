{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Event Network\n",
    "\n",
    "a. stein 7.28.2022\n",
    "\n",
    "Okay. So following `explore/drought_tracking.ipynb` I did `quality_control/tset_drought_event_plot.ipynb` and found that while blob identifying and connecting over time works ... the id system is a nightmare and not very smooth to use, especially as I realized during testing that I needed one extra set of parenthesis around a split to make the id's unique. This makes sorting them a further pain and there isn't a great way to trace history despite the id being unique (writing something to then process sorting is really annoying). So, let's try making a nodal network to keep track of it instead in combination with networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pool0/data/steinadi/.conda/envs/sedi/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import rasterio as rio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import ndrought.wrangle as wrangle\n",
    "import ndrought.compare as compare\n",
    "import ndrought.plotting as ndplot\n",
    "\n",
    "import skimage\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import regionprops_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventNode():\n",
    "\n",
    "    def __init__(self, time, area, coords, event_code):\n",
    "        self.time = time\n",
    "        self.area = area\n",
    "        self.coords = coords\n",
    "        self.event_code = event_code\n",
    "        self.future: List[EventNode] = list()\n",
    "\n",
    "    def __str__(self):\n",
    "        future_events = list()\n",
    "        for future_EventNode in self.future:\n",
    "            future_events.append(future_EventNode.event_code)\n",
    "        return f'time: {self.time}, area: {self.area}, futures: {future_events}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self\n",
    "        for node in self.future:\n",
    "            yield node\n",
    "\n",
    "    def append_future(self, other):\n",
    "        self.future.append(other)\n",
    "\n",
    "    def check_connects(self, other, auto_connect=True):\n",
    "        \n",
    "        connection_found = False\n",
    "\n",
    "        self_coord_set = set(tuple(coord) for coord in self.coords)\n",
    "        other_coord_set = set(tuple(coord) for coord in other.coords)\n",
    "\n",
    "        if len(self_coord_set.intersection(other_coord_set)) > 0:\n",
    "            connection_found = True\n",
    "            if auto_connect:\n",
    "                self.append_future(other)\n",
    "        \n",
    "        return connection_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I think above is a fairly good starting point for this class. I need to do the following:\n",
    "- try constructing a string of `EventNode`'s and see if they match a test version\n",
    "- figure out how to transition from nodes to timeseries\n",
    "- figure out how to hold all the nodes for a timeseries together, given they can stop and end and may not all be connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_drought_blob(vals:np.ndarray):\n",
    "    \"\"\"Using sci-kit image, identify drought event blobs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vals: np.ndarray\n",
    "        Spatial values for drought data categorized\n",
    "        according to the USDM scheme for a single\n",
    "        time step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Drought blobs using connectivity 2 from\n",
    "        skimage.measure.label. Blobs are binary\n",
    "        definitions of drought, where the measure\n",
    "        exceeds D1. Each blob is provided with\n",
    "        it's area, bbox, convex_area, and coordinates\n",
    "        of all cells contained within the blob.    \n",
    "    \"\"\"\n",
    "\n",
    "    # first we're going to make this binary\n",
    "    # by setting data in a drought to 1 and\n",
    "    # not in a drought to 0, including nan\n",
    "\n",
    "    vals[(vals < 1) | np.isnan(vals)] = 0\n",
    "    vals[vals > 0] = 1\n",
    "\n",
    "    # now we are going to convert to RGBL\n",
    "    (h, w) = vals.shape\n",
    "    t = (h, w, 3)\n",
    "    A = np.zeros(t, dtype=np.uint8)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            # since we already made it binary, this\n",
    "            # will make 1 vals be white and 0 vals\n",
    "            # be black in our RGB array\n",
    "            color_val = 255*vals[i,j]\n",
    "            A[i, j] = [color_val, color_val, color_val]\n",
    "\n",
    "    # connectivity 2 will consider diagonals as connected\n",
    "    blobs = skimage.measure.label(rgb2gray(A) > 0, connectivity=2)\n",
    "\n",
    "    properties =['area', 'coords']\n",
    "    df = pd.DataFrame(regionprops_table(blobs, properties=properties))\n",
    "    df['drought_id'] = np.nan*np.zeros(len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def connect_blobs_over_time(df_1:pd.DataFrame, df_2:pd.DataFrame):\n",
    "    \"\"\"Identify blobs shared between time frames.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    df_1 : pd.DataFrame\n",
    "        Blob dataframe at first time index.\n",
    "    df_2 : pd.DataFrame\n",
    "        Blob dataframe at second time index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Indices to each dataframe denoting which\n",
    "        blobs are shared, where each tuple in the\n",
    "        list is connection. The first index of\n",
    "        each tuple corresponds to df_1, while the\n",
    "        second index correponds to df_2\n",
    "    \"\"\"\n",
    "\n",
    "    blob_pairs = []\n",
    "\n",
    "    for idx_1, df_1_coords in enumerate(df_1.coords.values):\n",
    "        df_1_coords_set = set(tuple(coord) for coord in df_1_coords)\n",
    "        for idx_2, df_2_coords in enumerate(df_2.coords.values):\n",
    "            df_2_coords_set = set(tuple(coord) for coord in df_2_coords)\n",
    "            if len(df_1_coords_set.intersection(df_2_coords_set)) > 0:\n",
    "                blob_pairs.append((idx_1, idx_2))\n",
    "\n",
    "    return blob_pairs\n",
    "\n",
    "def propagate_drought_id(df_1=None, df_2=None, connections=[], new_blob_num=1):\n",
    "\n",
    "    if len(connections) > 0:\n",
    "\n",
    "        # need to keep track of splits among multiple\n",
    "        # blobs (since they are 1-to-many and we are\n",
    "        # iterating through linearly)\n",
    "        split_origins = dict()\n",
    "\n",
    "        for i in np.arange(len(df_2)):\n",
    "            drought_id = \"\"\n",
    "\n",
    "            # ALL CONNECTIONS\n",
    "            # first we need to figure out if we are connected\n",
    "            connects_origins = list()\n",
    "            for connect in connections:\n",
    "                # this means that our current index\n",
    "                # connects to a previous time's index\n",
    "                if connect[1] == i:\n",
    "                    # we already know it's going to index i\n",
    "                    # we need to figure out where it's coming from\n",
    "                    connects_origins.append(connect[0])\n",
    "\n",
    "            # SPLITS        \n",
    "            # now we need to check if this is part of a split\n",
    "            split_connections = dict()\n",
    "            for origin in connects_origins:\n",
    "                split_counter = 0\n",
    "                for connect in connections:\n",
    "                    # we want to count how many times the origin is\n",
    "                    # connected to something ... if it ends up being\n",
    "                    # more than once then it's a split\n",
    "                    if connect[0] == origin:\n",
    "                        split_counter += 1\n",
    "                # meaning we found a split\n",
    "                if split_counter > 1:\n",
    "                    split_connections[origin] = split_counter\n",
    "                    # if this is a new split we found, we\n",
    "                    # should make sure to save a note of it\n",
    "                    if origin not in split_origins.keys():\n",
    "                        split_origins[origin] = 1\n",
    "                \n",
    "            # so this would be if the split was found        \n",
    "            if len(split_connections) > 0:\n",
    "                for split_origin in split_connections.keys():\n",
    "                    split_origin_id = df_1['drought_id'].values[split_origin]\n",
    "                    current_split_num = split_origins[split_origin]\n",
    "\n",
    "                    drought_id = f'({split_origin_id})-{current_split_num}'\n",
    "                    \n",
    "                    # iterate for the next blob it splits into\n",
    "                    split_origins[split_origin] += 1\n",
    "\n",
    "            # MERGES\n",
    "            # we have a merge if more than 1 blob\n",
    "            # goes into this one\n",
    "            if len(connects_origins) > 1:\n",
    "                merged_blob_ids = df_1.iloc[connects_origins].sort_values('area', ascending=False)['drought_id'].values\n",
    "                # double check if we already had a split and began\n",
    "                # writing our code for this blob, if not we need to\n",
    "                # set it up\n",
    "                if len(drought_id) == 0:\n",
    "                    drought_id = merged_blob_ids[0]\n",
    "                for id in merged_blob_ids[1:]:\n",
    "                    drought_id = f'{drought_id}.({id})'\n",
    "                    \n",
    "            # NO SPLIT NO MERGE        \n",
    "            if len(connects_origins) == 1 and len(split_connections) == 0:\n",
    "                drought_id = df_1.iloc[connects_origins[0]]['drought_id']\n",
    "                \n",
    "\n",
    "            # CONNECTIONS EXIST, BUT NEW BLOB\n",
    "            if len(connects_origins) == 0:\n",
    "                drought_id = f'{new_blob_num}'\n",
    "                new_blob_num += 1    \n",
    "\n",
    "            df_2.loc[i, 'drought_id'] = drought_id                   \n",
    "\n",
    "    else:\n",
    "        # there were no connections, all id's start from scratch\n",
    "        for i in np.arange(len(df_2)):\n",
    "            df_2.loc[i, 'drought_id'] = f'{new_blob_num}'\n",
    "            new_blob_num += 1\n",
    "\n",
    "    return df_2, new_blob_num\n",
    "\n",
    "def encode_drought_events(data:np.ndarray):\n",
    "    \"\"\"Detect and encode drought events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Expecting first index to be temporal while second\n",
    "        and third are spatial.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A multi-indexed dataframe with time as the first level\n",
    "        and drought_id as the second level. 'area', 'convex_area',\n",
    "        and 'coords' are also outputted in this dataframe computed \n",
    "        from sci-kit image. \n",
    "    \n",
    "    \"\"\"\n",
    "    blob_dfs = []\n",
    "\n",
    "    for i in tqdm(np.arange(data.shape[0]), desc='Identifying Blobs'):\n",
    "        blob_dfs.append(identify_drought_blob(data[i,:,:]))\n",
    "    \n",
    "\n",
    "    #return blob_dfs\n",
    "\n",
    "    new_blob_num = 1\n",
    "    init_df, new_blob_num = propagate_drought_id(df_2=blob_dfs[0])\n",
    "    init_df['time'] = 0\n",
    "    encoded_blob_dfs = [init_df]    \n",
    "    for i in tqdm(np.arange(len(blob_dfs)-1), desc='Encoding Blobs'):\n",
    "        df_1 = encoded_blob_dfs[i]\n",
    "        df_2 = blob_dfs[i+1]\n",
    "\n",
    "        blob_pairs = connect_blobs_over_time(df_1, df_2)\n",
    "        df_2_encoded, new_blob_num = propagate_drought_id(df_1, df_2, blob_pairs, new_blob_num)\n",
    "        df_2_encoded['time'] = i+1\n",
    "        encoded_blob_dfs.append(df_2_encoded)\n",
    "\n",
    "    all_blobs_df = pd.concat([df[['time', 'drought_id', 'area', 'coords']] for df in encoded_blob_dfs], ignore_index=True)\n",
    "    all_blobs_df = all_blobs_df.set_index(['time', 'drought_id'])\n",
    "    all_blobs_df['drought_id'] = all_blobs_df.index.get_level_values(1)\n",
    "\n",
    "    return all_blobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_path = '/pool0/home/steinadi/data/drought/drought_impact/data/drought_measures'\n",
    "\n",
    "paired_ds = xr.open_dataset(f'{dm_path}/ndrought_products/paired_USDM_SPI.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccec843c6ba14363bca8be5e3f1d7fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Identifying Blobs:   0%|          | 0/1148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2c85ce1d714b7b98a9836d34b4a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding Blobs:   0%|          | 0/1147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USDM_events = encode_drought_events(paired_ds['USDM'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>convex_area</th>\n",
       "      <th>coords</th>\n",
       "      <th>drought_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>drought_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <th>1</th>\n",
       "      <td>653</td>\n",
       "      <td>665</td>\n",
       "      <td>[[0, 154], [0, 155], [0, 156], [0, 157], [0, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <th>1</th>\n",
       "      <td>653</td>\n",
       "      <td>665</td>\n",
       "      <td>[[0, 154], [0, 155], [0, 156], [0, 157], [0, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">58</th>\n",
       "      <th>1</th>\n",
       "      <td>12006</td>\n",
       "      <td>13514</td>\n",
       "      <td>[[0, 49], [0, 50], [0, 51], [0, 52], [0, 53], ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>65</td>\n",
       "      <td>[[6, 41], [7, 41], [7, 43], [7, 44], [7, 45], ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[10, 53]]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <th>(((75.(76).(77.(78)))-1.(80))-1)-2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[82, 62], [82, 63]]</td>\n",
       "      <td>(((75.(76).(77.(78)))-1.(80))-1)-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1145</th>\n",
       "      <th>(((75.(76).(77.(78)))-1.(80))-1)-1</th>\n",
       "      <td>6861</td>\n",
       "      <td>7561</td>\n",
       "      <td>[[0, 113], [0, 114], [0, 115], [0, 116], [0, 1...</td>\n",
       "      <td>(((75.(76).(77.(78)))-1.(80))-1)-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(((75.(76).(77.(78)))-1.(80))-1)-2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[82, 62], [82, 63]]</td>\n",
       "      <td>(((75.(76).(77.(78)))-1.(80))-1)-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <th>(((75.(76).(77.(78)))-1.(80))-1)-1</th>\n",
       "      <td>6771</td>\n",
       "      <td>7276</td>\n",
       "      <td>[[0, 114], [0, 115], [0, 116], [0, 117], [0, 1...</td>\n",
       "      <td>(((75.(76).(77.(78)))-1.(80))-1)-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <th>(((75.(76).(77.(78)))-1.(80))-1)-1</th>\n",
       "      <td>6810</td>\n",
       "      <td>7316</td>\n",
       "      <td>[[0, 113], [0, 114], [0, 115], [0, 116], [0, 1...</td>\n",
       "      <td>(((75.(76).(77.(78)))-1.(80))-1)-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          area  convex_area  \\\n",
       "time drought_id                                               \n",
       "56   1                                     653          665   \n",
       "57   1                                     653          665   \n",
       "58   1                                   12006        13514   \n",
       "     2                                      37           65   \n",
       "     3                                       1            1   \n",
       "...                                        ...          ...   \n",
       "1144 (((75.(76).(77.(78)))-1.(80))-1)-2      2            2   \n",
       "1145 (((75.(76).(77.(78)))-1.(80))-1)-1   6861         7561   \n",
       "     (((75.(76).(77.(78)))-1.(80))-1)-2      2            2   \n",
       "1146 (((75.(76).(77.(78)))-1.(80))-1)-1   6771         7276   \n",
       "1147 (((75.(76).(77.(78)))-1.(80))-1)-1   6810         7316   \n",
       "\n",
       "                                                                                    coords  \\\n",
       "time drought_id                                                                              \n",
       "56   1                                   [[0, 154], [0, 155], [0, 156], [0, 157], [0, 1...   \n",
       "57   1                                   [[0, 154], [0, 155], [0, 156], [0, 157], [0, 1...   \n",
       "58   1                                   [[0, 49], [0, 50], [0, 51], [0, 52], [0, 53], ...   \n",
       "     2                                   [[6, 41], [7, 41], [7, 43], [7, 44], [7, 45], ...   \n",
       "     3                                                                          [[10, 53]]   \n",
       "...                                                                                    ...   \n",
       "1144 (((75.(76).(77.(78)))-1.(80))-1)-2                               [[82, 62], [82, 63]]   \n",
       "1145 (((75.(76).(77.(78)))-1.(80))-1)-1  [[0, 113], [0, 114], [0, 115], [0, 116], [0, 1...   \n",
       "     (((75.(76).(77.(78)))-1.(80))-1)-2                               [[82, 62], [82, 63]]   \n",
       "1146 (((75.(76).(77.(78)))-1.(80))-1)-1  [[0, 114], [0, 115], [0, 116], [0, 117], [0, 1...   \n",
       "1147 (((75.(76).(77.(78)))-1.(80))-1)-1  [[0, 113], [0, 114], [0, 115], [0, 116], [0, 1...   \n",
       "\n",
       "                                                                 drought_id  \n",
       "time drought_id                                                              \n",
       "56   1                                                                    1  \n",
       "57   1                                                                    1  \n",
       "58   1                                                                    1  \n",
       "     2                                                                    2  \n",
       "     3                                                                    3  \n",
       "...                                                                     ...  \n",
       "1144 (((75.(76).(77.(78)))-1.(80))-1)-2  (((75.(76).(77.(78)))-1.(80))-1)-2  \n",
       "1145 (((75.(76).(77.(78)))-1.(80))-1)-1  (((75.(76).(77.(78)))-1.(80))-1)-1  \n",
       "     (((75.(76).(77.(78)))-1.(80))-1)-2  (((75.(76).(77.(78)))-1.(80))-1)-2  \n",
       "1146 (((75.(76).(77.(78)))-1.(80))-1)-1  (((75.(76).(77.(78)))-1.(80))-1)-1  \n",
       "1147 (((75.(76).(77.(78)))-1.(80))-1)-1  (((75.(76).(77.(78)))-1.(80))-1)-1  \n",
       "\n",
       "[1408 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USDM_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('sedi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5585d616cc037afe4e5997f9d5ad9938ec3148b25e01a6d2931d801949bec716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
